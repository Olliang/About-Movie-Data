{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory-based Collaborative Filtering Recommendation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will showcase how a Recommender System is built with one type of Collaborative Filtering techniques, which models the users' interests by collecting other users' preferences(collaborating) to provide personalized recommendations(filtering). The Collaborative Filtering techniques are commonly devided into two types: Memory-based and Model-based. <br>\n",
    "<br>\n",
    "**Memory-based CF:**<br>\n",
    "The rating matrix is directly used to calculate user/item similarity and make rating predictions. Finding nearest neighbors for predicting rating is the essence of this method.Two directions for the memory-based prediction are user-based similarity and item-based similarity. The former one predicts a rating by a user on a movie based on how other similar users predicted that movie, and the later one predicts the rating based on how other similar items are rated by that user. <br>\n",
    "<br>\n",
    "Here I visualized the similarity matrix calculation for both user-based and item-based approaches:\n",
    "\n",
    "![similarity matrix](Similarity_Matrix_Calculation.png)\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Model-based CF:**<br>\n",
    "The prediction relies on a offline learned model that is re-trained (updated) periodically. More information about this type of Collaborating Filtering will be provided in another notebook that showcase Matrix Factorization Recommender System building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cust_Id</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1:</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1488844</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>822109</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>885013</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30878</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cust_Id  Rating\n",
       "0       1:     NaN\n",
       "1  1488844     3.0\n",
       "2   822109     5.0\n",
       "3   885013     4.0\n",
       "4    30878     4.0"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load rating data\n",
    "df1 = pd.read_csv('combined_data_1.txt', header = None, names = ['Cust_Id', 'Rating'])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the scale of the dimensions in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of movies:  4499\n",
      "# of customers:  470758\n",
      "# of ratings:  24053764\n"
     ]
    }
   ],
   "source": [
    "# how many movies are there?\n",
    "n_movies = df1.isnull().sum()[1]\n",
    "print('# of movies: ',n_movies) # df.isnull().sum() will return the number oF missing values in each column.\n",
    "# how many customers are there?\n",
    "n_custs = df1[df1['Rating'].notnull()]['Cust_Id'].nunique()\n",
    "print('# of customers: ', n_custs)\n",
    "# How many ratings are there?\n",
    "n_ratings = df1[df1['Rating'].notnull()].count()[1]\n",
    "print('# of ratings: ', n_ratings) # df.count() will will return the number oF rows in each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olivi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Cust_Id  Rating  Movie_Id\n",
      "1  1488844     3.0         1\n",
      "2   822109     5.0         1\n",
      "3   885013     4.0         1\n",
      "4    30878     4.0         1\n",
      "5   823519     3.0         1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olivi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "def adjust_data(df1):\n",
    "    df1_na = pd.DataFrame(pd.isnull(df1.Rating))\n",
    "    df1_na = df1_na[df1_na['Rating'] == True]\n",
    "    df1_na = df1_na.reset_index() # the index of original ratings that are NA will be in a separate column\n",
    "\n",
    "    movieid_list = []\n",
    "    movie_id = 1\n",
    "\n",
    "    for i,j in zip(df1_na['index'][1:],df1_na['index'][:-1]):\n",
    "        temp = np.full((1,i-j-1), movie_id)   # np.full(shape, the number you want to fill in those shape)\n",
    "        movieid_list = np.append(movieid_list, temp)\n",
    "        movie_id += 1\n",
    "\n",
    "    # Add in the last record and corresponding length\n",
    "    last_record = np.full((1,len(df1) - df1_na.iloc[-1, 0] - 1),movie_id)\n",
    "    movieid_list = np.append(movieid_list, last_record)\n",
    "    df1 = df1[df1['Rating'].notnull()]\n",
    "    df1['Movie_Id'] = movieid_list.astype(int)\n",
    "    df1['Cust_Id'] = df1['Cust_Id'].astype(int)\n",
    "    return df1\n",
    "\n",
    "df1 = adjust_data(df1)\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove unessential data for faster computing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rating distribution of movies and users are both very right skewed, meaning that most users have only rated a very small number of movies and a lot of movies have relatively lower number of users rated. Those less active users or less popular movies can be dropped for significant improve in compute, because their ratings are less likely to be informative.\n",
    "\n",
    "What I am doing here:\n",
    "1. Remove movies with too less ratings (less popular movies)\n",
    "2. Remove customers with too less ratings (less active customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie minimum times of review: 1799.0\n",
      "Customer minimum times of review: 52.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f = ['count','mean']\n",
    "\n",
    "df_movie_summary = df1.groupby('Movie_Id')['Rating'].agg(f)\n",
    "df_movie_summary.index = df_movie_summary.index.map(int)\n",
    "movie_benchmark = round(df_movie_summary['count'].quantile(0.7),0)\n",
    "drop_movie_list = df_movie_summary[df_movie_summary['count'] < movie_benchmark].index\n",
    "print('Movie minimum times of review: {}'.format(movie_benchmark))\n",
    "\n",
    "df_cust_summary = df1.groupby('Cust_Id')['Rating'].agg(f)\n",
    "df_cust_summary.index = df_cust_summary.index.map(int)\n",
    "cust_benchmark = round(df_cust_summary['count'].quantile(0.7),0)\n",
    "drop_cust_list = df_cust_summary[df_cust_summary['count'] < cust_benchmark].index\n",
    "print('Customer minimum times of review: {}'.format(cust_benchmark))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dimensions of the dataset:  (17337458, 3)\n"
     ]
    }
   ],
   "source": [
    "df1 = df1[~df1['Movie_Id'].isin(drop_movie_list)]\n",
    "df1 = df1[~df1['Cust_Id'].isin(drop_cust_list)]\n",
    "print('New dimensions of the dataset: ',df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie_Id</th>\n",
       "      <th>Year</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Dinosaur Planet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>Isle of Man TT 2004 Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>Character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>Paula Abdul's Get Up &amp; Dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>The Rise and Fall of ECW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Movie_Id    Year                          Name\n",
       "0         1  2003.0               Dinosaur Planet\n",
       "1         2  2004.0    Isle of Man TT 2004 Review\n",
       "2         3  1997.0                     Character\n",
       "3         4  1994.0  Paula Abdul's Get Up & Dance\n",
       "4         5  2004.0      The Rise and Fall of ECW"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map with movie titles\n",
    "df_title =  pd.read_csv('movie_titles.csv', encoding = \"ISO-8859-1\", header = None, names = ['Movie_Id', 'Year', 'Name'])\n",
    "df_title.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborating Filtering Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UserId, MovieId, and Rating are the only 3 columns needed for collaborative filtering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17337458 entries, 696 to 24056846\n",
      "Data columns (total 3 columns):\n",
      "Cust_Id     int32\n",
      "Rating      float64\n",
      "Movie_Id    int32\n",
      "dtypes: float64(1), int32(2)\n",
      "memory usage: 396.8 MB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the limited memory my laptop allows, I will use a 0.2% subset of the original data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 34675 entries, 22240780 to 694150\n",
      "Data columns (total 3 columns):\n",
      "Cust_Id     34675 non-null int32\n",
      "Rating      34675 non-null float64\n",
      "Movie_Id    34675 non-null int32\n",
      "dtypes: float64(1), int32(2)\n",
      "memory usage: 812.7 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample 0.5% of the ratings dataset\n",
    "df_subset = df1.sample(frac=0.002)\n",
    "# Check the sample info\n",
    "print(df_subset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27740, 3)\n",
      "(6935, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "train_data, test_data = model_selection.train_test_split(df_subset, test_size=0.2)\n",
    "print(train_data.shape) \n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create user-item matrix for both training and test data respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24351, 1327)\n",
      "(6701, 1101)\n"
     ]
    }
   ],
   "source": [
    "train_matrix = pd.pivot_table(train_data,values='Rating',index='Cust_Id',columns='Movie_Id')\n",
    "test_matrix = pd.pivot_table(test_data,values='Rating',index='Cust_Id',columns='Movie_Id')\n",
    "\n",
    "\n",
    "print(train_matrix.shape)\n",
    "print(test_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Similarity Matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of different metrics to calculate the similarity such as Jaccard Similarity, Cosine Similarity, and Pearson Correlation. Particularly, Pearson Correlation is the best and safest way to calculate user-based similarity because it considered standardizing each user's ratings, in order to avoid situations like when the comparison is affected by different rating habits across users. But item-based similarity does not need the standardization because the ratings are all come from the same user. So item-based similarity matrix can be calculated by using either Cosine Similarity or Pearson Correlation.\n",
    "\n",
    "I will provide two solutions that both utilize Pearson Similarity as the metric for user-based . Method 1 uses the built-in function, while method 2 calculates the same function step by step mannually.\n",
    "\n",
    "\n",
    "- Calculating **user-based similarity matrix**\n",
    "![user_based](pearson_correlation.PNG)\n",
    "\n",
    "- Calculating **item-based similarity matrix**\n",
    "![item_based](cosine_similarity.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_similarity = train_matrix.corr(method = 'pearson')\n",
    "item_similarity = train_matrix.T.corr(method = 'pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid the impact of different rating style across users, I will normalize all the ratings of each user(user vector) before we calcuate the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(r):\n",
    "    result = (r - r.mean())/(r.max() - r.min())\n",
    "    return result\n",
    "\n",
    "train_matrix_normalized = train_matrix.apply(normalize)\n",
    "test_matrix_normalized = test_matrix.apply(normalize)\n",
    "train_matrix_normalized = train_matrix_normalized.fillna(0)\n",
    "test_matrix_normalized = test_matrix_normalized.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user-based similarity matrix:\n",
      "[[ 1.00000000e+00  3.97778111e-04  9.68736369e-05 -1.53646636e-03]\n",
      " [ 3.97778111e-04  1.00000000e+00  2.83623952e-03 -1.69078776e-03]\n",
      " [ 9.68736369e-05  2.83623952e-03  1.00000000e+00 -2.07810467e-03]\n",
      " [-1.53646636e-03 -1.69078776e-03 -2.07810467e-03  1.00000000e+00]]\n",
      "item-based similarity matrix:\n",
      "[[ 1.00000000e+00 -3.57839120e-04 -1.46643111e-04 -2.50134943e-04]\n",
      " [-3.57839120e-04  1.00000000e+00 -2.00435673e-04 -3.41891039e-04]\n",
      " [-1.46643111e-04 -2.00435673e-04  1.00000000e+00 -1.40107559e-04]\n",
      " [-2.50134943e-04 -3.41891039e-04 -1.40107559e-04  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "user_similarity = cosine_similarity(train_matrix_normalized)\n",
    "item_similarity = cosine_similarity(train_matrix_normalized.T)\n",
    "\n",
    "# output similarity matrix arrary \n",
    "print('user-based similarity matrix:')\n",
    "print(user_similarity[:4,:4])\n",
    "print('item-based similarity matrix:')\n",
    "print(item_similarity[:4,:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sim_df = pd.DataFrame(user_similarity, \n",
    "                           index = train_matrix_normalized.index, \n",
    "                           columns = train_matrix_normalized.index)\n",
    "item_sim_df = pd.DataFrame(item_similarity, \n",
    "                           index = train_matrix_normalized.columns, \n",
    "                           columns = train_matrix_normalized.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get similar users or items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Please don't take the predicted results in this notebook seriously , because the collaborative filtering model is very bad in scalability. Due to the limit of memory in my laptop, I needed to downsample the to only 0.2% of the original data, which significantly caused the sparsity of the table. So I apologize for this and please know that no meanningful result is very normal and reasonable in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cust_Id\n",
      "2453373    4.0\n",
      "1381920    4.0\n",
      "1589599    4.0\n",
      "607560     4.0\n",
      "618579     4.0\n",
      "Name: 424, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def get_similar_users(userid, rating):\n",
    "    similar_score = user_sim_df[userid]*(rating-2.5)\n",
    "    similar_score = similar_score.sort_values(ascending = False)\n",
    "    \n",
    "    return similar_score\n",
    "\n",
    "print(get_similar_users(424,4)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie_Id\n",
      "8       0.500000\n",
      "3147    0.062437\n",
      "256     0.036141\n",
      "2164   -0.000071\n",
      "774    -0.000071\n",
      "Name: 8, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def get_similar_movies(movie_id, rating):\n",
    "    similar_score = item_sim_df[movie_id]*(rating-2.5)\n",
    "    similar_score = similar_score.sort_values(ascending = False)\n",
    "    \n",
    "    return similar_score\n",
    "\n",
    "print(get_similar_movies(8,3)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie_Id</th>\n",
       "      <th>Year</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>What the #$*! Do We Know!?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>256</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>Ghost Dog: The Way of the Samurai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>586</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>Shanghai Triad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>774</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Foyle's War: Set 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>1179</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>The Education of Little Tree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364</th>\n",
       "      <td>1365</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>Kurt &amp; Courtney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1998</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>Saving Face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>2164</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>Rory O'Shea Was Here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2526</th>\n",
       "      <td>2527</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Quicksand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3146</th>\n",
       "      <td>3147</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>The Twilight Zone: Vol. 25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Movie_Id    Year                               Name\n",
       "7            8  2004.0         What the #$*! Do We Know!?\n",
       "255        256  2000.0  Ghost Dog: The Way of the Samurai\n",
       "585        586  1995.0                     Shanghai Triad\n",
       "773        774  2003.0                 Foyle's War: Set 2\n",
       "1178      1179  1997.0       The Education of Little Tree\n",
       "1364      1365  1998.0                    Kurt & Courtney\n",
       "1997      1998  2005.0                        Saving Face\n",
       "2163      2164  2004.0               Rory O'Shea Was Here\n",
       "2526      2527  2001.0                          Quicksand\n",
       "3146      3147  1964.0         The Twilight Zone: Vol. 25"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_title[df_title['Movie_Id'].isin(get_similar_movies(8,3)[:10].index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Because I have downsized the sample data to a relatively much smaller size and the extreme sparcity of this smaller data, it is very hard to be able to actually predict the results. So I will not run this code but leave it here for reference. \n",
    "\n",
    "The following function is using a common prediction function with a formula below:\n",
    "\n",
    "- For prediction from **user-based similarity matrix**:\n",
    "![formula](pred_user.PNG)\n",
    "\n",
    "- For prediction from **item-based similarity matrix**:\n",
    "![formula](pred_item.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the rating for a designated user on a designated item\n",
    "# note: the 'topn' is the threshold that restricts the number of the similar users\n",
    "def predict_rating(user, item, topn, type = 'user'):\n",
    "    if type == 'user':\n",
    "        avg_user_rating = train_matrix[user].mean()\n",
    "        similar_users = user_sim_df[user_sim_df.index!= user][user].sort_values(ascending = False)[:topn]\n",
    "        similar_userids = list(similar_users.index)\n",
    "        denominator = 1\n",
    "        for i in similar_users.values:\n",
    "            denominator *= i \n",
    "        # retrieve the rating of each users on the movie 8\n",
    "        numerator = 0\n",
    "        for j in similar_userids:\n",
    "            avg_rating = train_matrix.loc[j,:].mean()\n",
    "            weight = similar_users[j]\n",
    "            rating = train_matrix.loc[j,item]\n",
    "            if rating != np.nan:\n",
    "                numerator += weight*(rating-avg_rating)\n",
    "            else:\n",
    "                pass\n",
    "        result = avg_user_rating + (numerator/denominator)\n",
    "    elif type == 'item':\n",
    "        similar_items = item_sim_df[item_sim_df.index!= item][item].sort_values(ascending = False)[:topn]\n",
    "        similar_itemids = list(similar_items.index)\n",
    "        denominator = 1\n",
    "        for i in similar_users.values:\n",
    "            denominator *= i \n",
    "        numerator = 0\n",
    "        for j in similar_itemids:\n",
    "            weight = similar_items[j]\n",
    "            rating = trainmatrix.loc[user,j]\n",
    "            if rating != np.nan:\n",
    "                numerator += weight*rating\n",
    "            else:\n",
    "                pass\n",
    "        result = (numerator/denominator)\n",
    "    return round(result,2)\n",
    "\n",
    "def predict(test_data, topn, type = 'user'):\n",
    "    user_movies = test_data.as_matrix(columns = ['Cust_Id','Movie_Id'])\n",
    "    pred_result = []\n",
    "    if type = 'user':\n",
    "        for row in user_movies:\n",
    "            pred_result.append(predict_rating(row[0], row[1], topn, type = 'user'))\n",
    "    elif type = 'user':\n",
    "        for row in user_movies:\n",
    "            pred_result.append(predict_rating(row[0], row[1], topn, type = 'item'))\n",
    "    return pred_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many evaluation metrics but one of the most popular metric used to evaluate accuracy of predicted numerical values is Root Mean Squared Error (RMSE). I will use the mean_square_error (MSE) function from sklearn, where the RMSE is just the square root of MSE.\n",
    "\n",
    "![rmse](rmse.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def rmse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(pred, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF RMSE: 512635.12566851435\n",
      "Item-based CF RMSE: 626677.1369669506\n"
     ]
    }
   ],
   "source": [
    "test_rating = np.array(test_data['Rating'])\n",
    "# Predict ratings on the training data with both similarity score\n",
    "user_prediction = predict(test_data, 50, type='user')\n",
    "item_prediction = predict(test_data, 50, type='item')\n",
    "\n",
    "# RMSE on the test data\n",
    "print('User-based CF RMSE: ' + str(rmse(user_prediction, test_rating)))\n",
    "print('Item-based CF RMSE: ' + str(rmse(item_prediction, test_rating)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF RMSE: 361634.0427684085\n",
      "Item-based CF RMSE: 6627.045366193754\n"
     ]
    }
   ],
   "source": [
    "train_rating = np.array(train_data['Rating'])\n",
    "# RMSE on the train data\n",
    "print('User-based CF RMSE: ' + str(rmse(user_prediction, train_rating)))\n",
    "print('Item-based CF RMSE: ' + str(rmse(item_prediction, train_rating)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the implementation above, we can see some issues and challenges with memory-based Collaborating Filtering:\n",
    "\n",
    "1. **Handling unknown users/items (Cold Start Problem)**\n",
    "New users and movies have no history in the data, so this model is not able to predict the rating of those objects.<br>\n",
    "\n",
    "2. **Data Sparsity**\n",
    "The rating distribution of most users and movies are very right-skewed, which means there will be a huge sparsity in the rating matrix that affects the prediction accuracy. <br>\n",
    "\n",
    "3. **Scalability**\n",
    "The compute for similarity matrix is very expensive out of the nature huge matrix. <br>\n",
    "\n",
    "4. **Dynamic Update**\n",
    "The calculation purely depends on historical data, which contains ratings done in a wide range of period. So the result is hard to reflect the dynamic change of users' taste."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
